#!/usr/bin/env python3
"""
Ex√©cute s√©quentiellement : chargement ‚Üí validation ‚Üí pr√©traitement ‚Üí feature engineering
"""

import os
import sys
import time
import argparse
import pandas as pd
import mlflow
import mlflow.sklearn
from posthog import project_root
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, precision_score, recall_score,
    f1_score, roc_auc_score
)
from xgboost import XGBClassifier

# === Correction du chemin d'import pour les modules locaux ===
# IMPORTANT : permet d'importer correctement les modules depuis le dossier src/
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Modules locaux - composants cl√©s du pipeline
from src.data.load_data import load_data                    # Chargement des donn√©es avec gestion d'erreurs
from src.data.preprocess import preprocess_data            # Nettoyage de base
from src.features.build_features import build_features     # Feature engineering (CRITIQUE pour la performance)
from src.utils.validate_data import validate_telco_data    # Validation qualit√© des donn√©es


def main(args):
    """
    Fonction principale d'entra√Ænement qui orchestre le workflow ML complet.
    """

    # ==========================================================
    # CONFIGURATION MLFLOW ‚Äì SUIVI D‚ÄôEXP√âRIENCES (EXPERIMENT TRACKING)
    # ==========================================================
    # MLflow sert ici √† tracer chaque entra√Ænement sous forme de "run".
    # Un run MLflow enregistre :
    # - les param√®tres (hyperparam√®tres, seuil, test_size, etc.)
    # - les m√©triques (precision, recall, roc_auc, temps, etc.)
    # - les artefacts (mod√®le, fichiers JSON, pkl, etc.)
    #
    # Cela permet :
    # - la reproductibilit√©
    # - la comparaison de runs
    # - l‚Äôauditabilit√© (important en MLOps)
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    mlruns_path = args.mlflow_uri or f"file://{project_root}/mlruns"  # Tracking local (fichiers), pas serveur
    mlflow.set_tracking_uri(mlruns_path)

    # Un "experiment" est un conteneur logique de runs (ex: "Telco Churn")
    # Si l'experiment n'existe pas, MLflow le cr√©e automatiquement.
    mlflow.set_experiment(args.experiment)

    # D√©marrage d‚Äôun run MLflow : tout ce qui est logg√© dans ce bloc
    # sera rattach√© √† ce run (m√™mes m√©triques, m√™mes artefacts, etc.)
    with mlflow.start_run():

        # ==========================================================
        # JOURNALISATION DES PARAM√àTRES (MLflow)
        # ==========================================================
        # On log les param√®tres cl√©s pour reproduire l'exp√©rience plus tard
        mlflow.log_param("model", "xgboost")            # Type de mod√®le
        mlflow.log_param("threshold", args.threshold)  # Seuil de classification utilis√©
        mlflow.log_param("test_size", args.test_size)  # Ratio train/test

        # ==========================================================
        # √âTAPE 1 : CHARGEMENT + VALIDATION QUALIT√â
        # ==========================================================
        print("üîÑ Chargement des donn√©es...")
        df = load_data(args.input)
        print(f"‚úÖ Donn√©es charg√©es : {df.shape[0]} lignes, {df.shape[1]} colonnes")

        # Validation qualit√© : on bloque le training si les donn√©es ne sont pas conformes
        print("üîç Validation de la qualit√© des donn√©es (Great Expectations)...")
        is_valid, failed = validate_telco_data(df)

        # On log une m√©trique binaire : 1 si la qualit√© passe, 0 sinon
        # Cela permet de suivre dans le temps la stabilit√© de la qualit√© des donn√©es.
        mlflow.log_metric("data_quality_pass", int(is_valid))

        if not is_valid:
            # En cas d'√©chec : on log la liste des contr√¥les √©chou√©s en artefact
            # (tr√®s utile pour debug / audit)
            import json
            mlflow.log_text(
                json.dumps(failed, indent=2),
                artifact_file="failed_expectations.json"
            )
            raise ValueError(f"‚ùå Contr√¥le qualit√© KO. Probl√®mes : {failed}")
        else:
            print("‚úÖ Validation OK. R√©sultat logg√© dans MLflow.")

        # ==========================================================
        # √âTAPE 2 : PR√âTRAITEMENT
        # ==========================================================
        print("üîß Pr√©traitement des donn√©es...")
        df = preprocess_data(df)

        # Sauvegarde du dataset pr√©trait√© pour reproductibilit√© / debug
        processed_path = os.path.join(project_root, "data", "processed", "telco_churn_processed.csv")
        os.makedirs(os.path.dirname(processed_path), exist_ok=True)
        df.to_csv(processed_path, index=False)
        print(f"‚úÖ Dataset pr√©trait√© sauvegard√© : {processed_path} | Shape : {df.shape}")

        # ==========================================================
        # √âTAPE 3 : FEATURE ENGINEERING (CRITIQUE)
        # ==========================================================
        print("üõ†Ô∏è  Construction des features...")
        target = args.target
        if target not in df.columns:
            raise ValueError(f"Colonne cible '{target}' introuvable dans les donn√©es")

        # Encodage binaire + one-hot encoding
        df_enc = build_features(df, target_col=target)

        # S√©curit√© : conversion des bool√©ens en int pour compatibilit√© XGBoost
        for c in df_enc.select_dtypes(include=["bool"]).columns:
            df_enc[c] = df_enc[c].astype(int)
        print(f"‚úÖ Feature engineering termin√© : {df_enc.shape[1]} features")

        # ==========================================================
        # SAUVEGARDE DES M√âTADONN√âES DE FEATURES (COH√âRENCE SERVING)
        # ==========================================================
        # Objectif : garantir que l‚Äôinf√©rence (API) utilisera EXACTEMENT
        # les m√™mes colonnes et dans le m√™me ordre que pendant le training.
        import json, joblib
        artifacts_dir = os.path.join(project_root, "artifacts")
        os.makedirs(artifacts_dir, exist_ok=True)

        feature_cols = list(df_enc.drop(columns=[target]).columns)

        # Sauvegarde locale (utile pour dev / debug)
        with open(os.path.join(artifacts_dir, "feature_columns.json"), "w") as f:
            json.dump(feature_cols, f)

        # Log MLflow (artefact) : r√©cup√©rable depuis l'UI MLflow
        mlflow.log_text("\n".join(feature_cols), artifact_file="feature_columns.txt")

        # Artefact pkl : sert de ‚Äúcontrat‚Äù entre training et serving
        preprocessing_artifact = {
            "feature_columns": feature_cols,
            "target": target
        }
        joblib.dump(preprocessing_artifact, os.path.join(artifacts_dir, "preprocessing.pkl"))

        # On log √©galement ce fichier dans MLflow pour pouvoir le r√©cup√©rer en prod
        mlflow.log_artifact(os.path.join(artifacts_dir, "preprocessing.pkl"))
        print(f"‚úÖ Sauvegarde de {len(feature_cols)} colonnes de features pour la coh√©rence du serving")

        # ==========================================================
        # √âTAPE 4 : SPLIT TRAIN / TEST
        # ==========================================================
        print("üìä D√©coupage des donn√©es...")
        X = df_enc.drop(columns=[target])
        y = df_enc[target]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=args.test_size,
            stratify=y,
            random_state=42
        )
        print(f"‚úÖ Train : {X_train.shape[0]} √©chantillons | Test : {X_test.shape[0]} √©chantillons")

        # ==========================================================
        # GESTION DU D√âS√âQUILIBRE DE CLASSES
        # ==========================================================
        # scale_pos_weight ajuste l'importance de la classe minoritaire (churners)
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        print(f"üìà Ratio de d√©s√©quilibre : {scale_pos_weight:.2f} (appliqu√© √† la classe positive)")

        # ==========================================================
        # √âTAPE 5 : ENTRA√éNEMENT DU MOD√àLE
        # ==========================================================
        print("ü§ñ Entra√Ænement du mod√®le XGBoost...")

        model = XGBClassifier(
            n_estimators=301,
            learning_rate=0.034,
            max_depth=7,
            subsample=0.95,
            colsample_bytree=0.98,
            n_jobs=-1,
            random_state=42,
            eval_metric="logloss",
            scale_pos_weight=scale_pos_weight
        )

        # Mesure du temps d'entra√Ænement (performance)
        t0 = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - t0

        # Log du temps d'entra√Ænement dans MLflow (m√©trique)
        mlflow.log_metric("train_time", train_time)
        print(f"‚úÖ Mod√®le entra√Æn√© en {train_time:.2f} secondes")

        # ==========================================================
        # √âTAPE 6 : √âVALUATION
        # ==========================================================
        print("üìä √âvaluation des performances...")

        t1 = time.time()
        proba = model.predict_proba(X_test)[:, 1]

        # Application du seuil (plus bas = recall ‚Üë / precision ‚Üì)
        y_pred = (proba >= args.threshold).astype(int)
        pred_time = time.time() - t1

        # Log du temps d'inf√©rence dans MLflow (m√©trique)
        mlflow.log_metric("pred_time", pred_time)

        # Calcul des m√©triques
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, proba)

        # ==========================================================
        # LOG DES M√âTRIQUES DANS MLFLOW
        # ==========================================================
        # Ces m√©triques permettront de comparer plusieurs runs dans l‚ÄôUI MLflow.
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("roc_auc", roc_auc)

        print(f"üéØ Performances :")
        print(f"   Precision : {precision:.3f} | Recall : {recall:.3f}")
        print(f"   F1 Score : {f1:.3f} | ROC AUC : {roc_auc:.3f}")

        # ==========================================================
        # √âTAPE 7 : SAUVEGARDE DU MOD√àLE (MLFLOW)
        # ==========================================================
        print("üíæ Sauvegarde du mod√®le dans MLflow...")

        # mlflow.sklearn.log_model :
        # - s√©rialise le mod√®le
        # - cr√©e un dossier d'artefacts "model/"
        # - permet ensuite :
        #   - de r√©cup√©rer le mod√®le depuis MLflow
        #   - de servir le mod√®le via une API
        #   - de l'enregistrer en Model Registry (si activ√©)
        mlflow.sklearn.log_model(
            model,
            artifact_path="model"
        )
        print("‚úÖ Mod√®le sauvegard√© dans MLflow (artefact)")

        # ==========================================================
        # R√âSUM√â FINAL
        # ==========================================================
        print(f"\n‚è±Ô∏è  R√©sum√© performance :")
        print(f"   Temps entra√Ænement : {train_time:.2f}s")
        print(f"   Temps inf√©rence    : {pred_time:.4f}s")
        print(f"   Samples / seconde  : {len(X_test)/pred_time:.0f}")

        print(f"\nüìà Rapport d√©taill√© :")
        print(classification_report(y_test, y_pred, digits=3))


if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Ex√©cuter le pipeline churn avec XGBoost + MLflow")
    p.add_argument("--input", type=str, required=True,
                   help="chemin vers le CSV (ex: data/raw/Telco-Customer-Churn.csv)")
    p.add_argument("--target", type=str, default="Churn")
    p.add_argument("--threshold", type=float, default=0.35)
    p.add_argument("--test_size", type=float, default=0.2)
    p.add_argument("--experiment", type=str, default="Telco Churn")
    p.add_argument("--mlflow_uri", type=str, default=None,
                   help="surcharge l'URI MLflow, sinon utilise project_root/mlruns")

    args = p.parse_args()
    main(args)

"""
# Exemple d'ex√©cution du pipeline :

python scripts/run_pipeline.py \
    --input data/raw/Telco-Customer-Churn.csv \
    --target Churn

"""
